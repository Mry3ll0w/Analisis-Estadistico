---
title: "A3"
author: "Antonio Roldán Andrade"
date: "2025-12-15"
output: html_document
---

```{r setup, include=FALSE}
library(dplyr)
library(corrplot)
```

## 1 Regresión Lineal 

### 1.1 Carga y preparación de los datos

```{r, echo=TRUE}

csv_location <- "salarios_INE_2022_sample.csv"
salariosDataSet <- read.csv(csv_location, sep = ",")

head(salariosDataSet,5)
salariosDataSet <- salariosDataSet %>%
 mutate(
  # 1. CONVERSIÓN Y ETIQUETADO DE VARIABLES CATEGÓRICAS SIMPLES

 
  SEXO = factor(SEXO,
         levels = c(1, 6), # 1=Hombre, 6=Mujer
         labels = c("Hombre", "Mujer")),

  # ESTU (Nivel de Estudios): Sobrescribe la columna 'ESTU'
  ESTU = factor(ESTU,
         levels = c(1, 2, 3, 4, 5, 6),
         labels = c("1. Sin Estudios/Primaria Incompleta",
              "2. Primaria Completa",
              "3. Secundaria 1ª Etapa (E.S.O.)",
              "4. Secundaria 2ª Etapa (Bachillerato)",
              "5. Formación Profesional",
              "6. Estudios Superiores")),

  # ANOS2 (Grupos de Edad): Sobrescribe la columna 'ANOS2'
  ANOS2 = factor(ANOS2,
         levels = c(1, 2, 3, 4, 5, 6),
         labels = c("1. Menos de 19",
               "2. 20 a 29",
               "3. 30 a 39",
               "4. 40 a 49",
               "5. 50 a 59",
               "6. 60 y más"
              )),


  # 2. RECODIFICACIÓN ESPECÍFICA DE CNACE (Actividad Económica)
 
  CNACE = case_when(
   # Nivel 1: B, C, D, E, F
   CNACE %in% c("B", "C", "D", "E", "F") ~ "1. Industria y Construcción",
   # Nivel 2: O, P, Q
   CNACE %in% c("O", "P", "Q") ~ "2. Servicios Públicos Clave",
   # Nivel 3: El resto
   TRUE ~ "3. Otros Servicios y Actividades"
  ),

  # Creamos la nueva variable
  CNACE_grp = factor(CNACE,
         levels = c("1. Industria y Construcción",
               "2. Servicios Públicos Clave",
               "3. Otros Servicios y Actividades")),


  # 3. RECODIFICACIÓN ESPECÍFICA DE CNO1 
 
  CNO1 = case_when(
   # Nivel 1: A0–C0 (Directores y Técnicos/Profesionales)
   substr(CNO1, 1, 1) %in% c("A", "B", "C") ~ "1. Dirección y Alta Cualificación",
   # Nivel 2: D0–J0 (Técnicos de Apoyo, Empleados, Servicios, Agricultores)
   substr(CNO1, 1, 1) %in% c("D", "E", "F", "G", "H", "I", "J") ~ "2. Cualificación Media",
   # Nivel 3: El resto (K0, L0, M0, N0, O0, P0, Q0)
   TRUE ~ "3. Baja Cualificación y Elementales"
  ),

  # Convertir CNO1 a factor y ordenar los niveles
  CNO1_grp = factor(CNO1,
         levels = c("1. Dirección y Alta Cualificación",
              "2. Cualificación Media",
              "3. Baja Cualificación y Elementales"))
 )



``` 



## 1.2 Estudio de correlación lineal

```{r, echo=TRUE}
# Extraemos todas las variables cuantitativas de la tabla
datos_cuantitativos <- salariosDataSet %>%
  select(where(is.numeric))

# Una vez tenemos los datos procedemos a realizar la matriz de correlacion
matriz_correlacion <- cor(datos_cuantitativos, use = "pairwise.complete.obs")
# 3. Visualizar la matriz
corrplot(matriz_correlacion, 
         method = "circle", # Puedes usar "square" o "number"
         type = "upper",    # Muestra solo la mitad superior
         tl.cex = 0.7,      # Tamaño de la fuente de las etiquetas
         tl.col = "black",  # Color de las etiquetas
         tl.srt = 45,       # Rotación de las etiquetas
         addCoef.col = "black", # Añade el número del coeficiente (opcional)
         number.cex = 0.6)      # Tamaño de los números (si los añades)
```
Como podemos apreciar la matriz es enorme, por lo que realizaremos un filtrado por elementos con una correlación alta (>= 0.7), con esto tendremos una mayor facilidad para obtener un resultado estudiable

```{r, echo=TRUE}

correlaciones_df <- as.data.frame(matriz_correlacion) %>%
  tibble::rownames_to_column("Var1") %>%
  # Nos vamos moviendo para tener una fila por cada par de correlación
  tidyr::pivot_longer(cols = -Var1, names_to = "Var2", values_to = "Correlacion") %>%
  
  # Aseguramos que no comparemos Var1 vs Var2 y Var2 vs Var1, y que no sea la diagonal (Var1 == Var2)
  filter(Var1 < Var2) %>%
  
  # Buscar correlaciones fuertes (|r| > 0.7)
  filter(abs(Correlacion) > 0.7) %>%
  
  # Ordenar por el valor absoluto de la correlación (de más fuerte a más débil)
  arrange(desc(abs(Correlacion)))

# Imprimir las correlaciones más fuertes
print("Pares de variables con correlación |r| > 0.7:")
print(correlaciones_df)

```

Podemos apreciar la existencia de múltiples variables con una fuerte correlación pudiendo destacar:  

- BASE <-> COTIZA: Como es lógico existe una fuerte correlación entre las contribuciones a la seguridad social y la base de cotización a la seguridad social, existe una correlación positiva entre ambas casi completa (0.953).

- JAP <-> JSP1: Cuanto mayor es la jornada laboral anual pactada mayor será la jornada semanal en la mayoría de los casos.

- IRPF <-> RETRINOIN: Consecuentemente, cuanto mayor es el salario bruto anual mayor será la cantidad de IRPF que tendrá que pagar el trabajador.


### 1.4. Generación de los conjuntos de entrenamiento y de test


```{r, echo=TRUE}
# Fijar la semilla para reproducibilidad
set.seed(42)

#`1. Crear el vector de índices para la muestra de entrenamiento (80%) ---

tamano_train <- floor(0.80 * nrow(salariosDataSet))

# seleccionamos los índices de las filas que irán a 'train'
indices_train <- sample(seq_len(nrow(salariosDataSet)), size = tamano_train)

# --- 2. Separar los conjuntos de datos ---

train <- salariosDataSet[indices_train, ]

test <- salariosDataSet[-indices_train, ]

print(paste("Tamaño total:", nrow(salariosDataSet)))
print(paste("Tamaño train (80%):", nrow(train)))
print(paste("Tamaño test (20%):", nrow(test)))

```

### 1.5. Estimación de modelos de regresión lineales simples con variable cualitativa

```{r, echo=TRUE}

# M1: RETRINOIN en función de SEXO
# Usamos Lm ya que estamos en un estudio de regresión lineal simple
modelo_sexo <- lm(RETRINOIN ~ SEXO, data = train)

# Mostrar el resumen del modelo 1
summary(modelo_sexo)

```
__Como podemos apreciar en el resultado anterior, tenemos:__  
1) El salario medio estimado del grupo de referencia (hombre, b0) es de $31053.9$  
2) La diferencia salarial media estimada de las mujeres respecto a los hombres es de $-6472.0$  
3) El salario medio del grupo menos favorecido (mujeres) es:  
$$\text{Salario}_{\text{Resto del Mundo}} = \text{Intercepto} + \text{Coeficiente}_{\text{TIPOPAIS2}}$$

__Con respecto a la si estas estadísticas son significativas tenemos:__  

Puesto que el p-valor del coeficiente es <2e-16, esto implica que dado que es un valor extremadamente pequeño y es mucho menor 
que el umbral de significación (alpha = 0.05) por tanto podemos rechazar la hipotesis nula (Ho: coeficiente es 0).

Por tanto podemos decir que la diferencia salarial media estimada de $-6472.0$ es estadísticamente significativa. 

__Si nos centramos en la interpretación de $R^2$ tenemos:__
El valor obtenido R-Squared es del 1.731 %, por lo que podemos decir la variable SEXO influye únicamente en el $1.73%$ de la variabilidad de la varianza pudiendo asumir que aún siendo la diferencia salarial es estadísticamente significativa, el modelo es débil desde el punto de vista predictivo. La mayor parte de la variación salarial ($\approx 98.27\%$) se debe a otros factores no incluidos en este modelo simple.




```{r, echo=TRUE}
modelo_pais <- lm(RETRINOIN ~ TIPOPAIS, data = train)

# Mostrar el resumen del modelo 1
summary(modelo_pais)

```

__Como podemos apreciar en el resultado del segundo caso, tenemos:__  
1) El salario medio estimado del grupo de referencia (TIPOPAIS 1: España) es de $34080$.  
2) La diferencia salarial media estimada de los trabajadores con nacionalidad Resto del Mundo (TIPOPAIS 2) respecto a los de España es de $-5547$.  
3) El salario medio del grupo menos favorecido (Resto del Mundo) es:

$$\text{Salario}_{\text{Resto del Mundo}} = \text{Salario}_{\text{Medio España}} + \text{Coeficiente}_{\text{TPAIS}} = 34080 + (-5547) = 28533$$
__Con respecto a si estas estadísticas son significativas, tenemos:__  

Puesto que el $p$-valor del coeficiente es $2.2\text{e-}07$, esto implica que dado que es un valor extremadamente pequeño y es mucho menor que el umbral de significación ($\alpha = 0.05$), por lo tanto podemos rechazar la hipótesis nula ($H_0$: el coeficiente es 0).  
Por lo tanto, podemos decir que la diferencia salarial media estimada de $-5547$ es estadísticamente significativa.  

__Si nos centramos en la interpretación de $R^2$ tenemos:__  
El valor obtenido $R^2$ Ajustado es del $0.3227\%$ ($0.003227$), por lo que podemos decir la variable $TIPOPAIS$ influye únicamente en el $0.3227\%$ de la variabilidad de la varianza de la retribución, pudiendo asumir que aun siendo la diferencia salarial estadísticamente significativa, el modelo es extremadamente débil desde el punto de vista predictivo.  
La mayor parte de la variación salarial ($\approx 99.68\%$) se debe a otros factores no incluidos en este modelo simple.

### 1.6 Estimación del modelo de regresión lineal múltiple con predictores cuantitativos

Haremos ahora las regresiones lineales para la variables RETROIN, ANOANTI y JAP:

```{r, echo=TRUE}

modelo_antiguedad_jornada <- lm(RETRINOIN ~ ANOANTI + JAP, data = train)

summary(modelo_antiguedad_jornada)

```

Del resultado mostrado en la regresión lineal podemos inferir:  

1) La variable independiente representa la retribución de un trabajador con 0 experiencia y 0 horas de jornada laboral anual, esto nos ayuda para los cálculos posteriores.  

2) La variable ANOANTI nos indica que por cada año que pasa la retribución aumenta $712.9$ euros.  

3) Por cada hora adicional pactada anualmente la retribución aumenta $16,36$ euros.  

__Interpretación de $R^2$ Ajustado__  

1) El $R^2$ Ajustado del $14.9\%$ es sustancialmente mayor que los modelos simples anteriores (M1 con $\approx 1.73\%$ y M2 con $\approx 0.32\%$).  

2) Confirmando que las variables cuantitativas de "cantidad de trabajo y experiencia" son, efectivamente, mejores predictores de la retribución que las variables categóricas de sexo o nacionalidad.  

3) El modelo sigue siendo moderadamente débil, ya que el $85.1\%$ restante de la variabilidad en el salario se explica por otros factores.

































